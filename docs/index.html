<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">

<head>
  <meta charset="utf-8">
  <title>Bias 2020 - International Workshop on Algorithmic Bias in Search and Recommendation</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons -->
  <link href="img/favicon.png" rel="icon">
  <link href="img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Raleway:300,400,500,700,800" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">
  <link href="lib/venobox/venobox.css" rel="stylesheet">
  <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="css/style.css" rel="stylesheet">

</head>

<body>

  <!--==========================
    Header
  ============================-->
  <header id="header">
    <div class="container">

      <div id="logo" class="pull-left">
        <h1><a href="#intro">Bias 2020</a></h1>
      </div>

      <nav id="nav-menu-container">
        <ul class="nav-menu">
          <li><a href="#aims">Aims and Scope</a></li>
          <li><a href="#si">Special Issue</a></li>
          <li><a href="#report">Report</a></li>
          <li><a href="#attending">Keynote</a></li>
          <li><a href="#program">Program</a></li>
          <li><a href="#dates">Dates</a></li>
          <li><a href="#topics">Topics</a></li>
          <li><a href="#submission">Submission</a></li>
          <li><a href="#committee">Committee</a></li>
          <li><a href="#contacts">Contacts</a></li>
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->

  <!--==========================
    Intro Section
  ============================-->
  <section id="intro">
    <div class="intro-container wow fadeIn">
      <h1 class="mb-4 pb-0">International Workshop on <br/> Algorithmic Bias in Search and Recommendation (Bias 2020)</h1>
      <p class="mb-4 pb-0">to be held as part of the <u><a href="https://ecir2020.org/" target="_blank">42nd European Conference on Information Retrieval (ECIR 2020)</a></u></p>
      <p class="mb-4 pb-0">April 14, 2020 - ONLINE </p>
	  <div style="margin: 10px auto 30px auto; width: 50%; border: 3px solid #fff;">
		<h2 style="color:#fff; text-decoration: underline;">Announcement: </br> <a href="https://www.journals.elsevier.com/information-processing-and-management/call-for-papers/special-issue-on-algorithmic-bias-and-fairness-in-search" target="_blank">Information Processing & Management Special Issue</p></h2>
		<p style="text-decoration: underline; color: #fff;"><a href="https://www.journals.elsevier.com/information-processing-and-management/call-for-papers/special-issue-on-algorithmic-bias-and-fairness-in-search" target="_blank">https://www.journals.elsevier.com/information-processing-and-management/call-for-papers/special-issue-on-algorithmic-bias-and-fairness-in-search</a></p>
	  </div>
    </div>
  </section>

  <main id="main">

    <!--==========================
    Aims and Scope Section
    ============================-->
    <section id="aims" class="wow fadeInUp">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Aims and Scope</h2>
          <p><strong>Search</strong> and <strong>recommendation</strong> are getting closer and closer as research areas. Though they require fundamentally different inputs, i.e., the user is asked to provide a query in search, while implicit and
             explicit feedback is leveraged in recommendation, existing search algorithms are being personalized based on users' profiles and recommender systems are optimizing their output on the ranking quality.</p>
          <p>Both classes of algorithms aim to learn patterns from historical data that conveys <strong>biases</strong> in terms of <strong>imbalances</strong> and <strong>inequalities</strong>. These hidden biases
             are unfortunately captured in the learned patterns, and often emphasized in the results these algorithms provide to users. When a bias affects a <strong>sensitive attribute</strong> of a user, such as their gender or
              religion, the inequalities that are reinforced by search and recommendation algorithms even lead to severe <strong>societal consequences</strong>, like users discrimination.</p>
          <p>For this critical reason, being able to <strong>detect</strong>, <strong>measure</strong>, <strong>characterize</strong>, and <strong>mitigate</strong> these biases while keeping high effectiveness is a prominent and timely topic for the IR community. Mitigating the
             effects generated by popularity bias, ensuring results that are fair with respect to the users, and being able to interpret why a model provides a given recommendation or search result are examples of
             challenges that may be important in real-world applications. This workshop aims to collect new contributions in this emerging field and to provide a common ground for <strong>interested researchers</strong> and
             <strong>practitioners</strong>.</p>
        </div>
      </div>

    </section>
	
    <section id="si" class="wow fadeInUp section-with-bg" style="padding: 60px 0 30px 0;">

      <div class="container-fluid">
        <div class="section-header">
          <h2 style="margin: 10px auto 30px auto;">Special Issue</h2>
			<p>We have arranged a Special Issue, devoted to the workshop topics, on the journal "<strong>Information Processing & Management</strong>" (Elsevier; Impact Factor: 3.892). We solicit different types of contributions (research papers, surveys, replicability and reproducibility studies, resource papers, systematic review articles) on bias and fairness in search and recommendation.</p> 
			
			<p><strong>Details</strong>: <a href="https://www.journals.elsevier.com/information-processing-and-management/call-for-papers/special-issue-on-algorithmic-bias-and-fairness-in-search" target="_blank">Call for Papers of the Special Issue</a> on the journal website. </p>
        </div>
      </div>

    </section>
	
    <section id="report" class="wow fadeInUp" style="padding: 60px 0 30px 0;">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Report</h2>
          <p>On April 14, 2020, <strong>the International Workshop on Algorithmic Bias in Search and Recommendation</strong> was held online, in conjunction with the 42nd European Conference on Information Retrieval (ECIR 2020). The workshop had more than <strong>70 </strong>participants. The keynote speaker was <strong>Prof. Chirag Shah</strong> from the University of Washington (United States). The <strong>scientific program</strong> included demo and paper presentations. The papers covered topics that go from search and recommendation in online dating, education, and social media, over the impact of gender bias in word embeddings, to tools that allow to explore bias and fairness on the Web. The event concluded with a <strong>discussion session</strong> aimed at highlighting open issues and research challenges.</p>
		  <p><strong>Download</strong>: <a href="https://drive.google.com/open?id=1uznCN3lE0tFNyonh84qcRdTGxKaU6SLk" target="_blank">Full Report</a> (to appear on the June 2020 issue of SIGIR Forum)</p>
        </div>
      </div>

    </section>

    <!--==========================
    Keynote Section
    ============================-->
    <section id="attending" class="wow fadeInUp section-with-bg">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Keynote</h2>
		  
		     <p style="text-align: center;"><img src="http://chiragshah.org/images/profile.png" class="img-rounded" alt="Chirag Shah" style="width:350px;"></p>
		  
			 <p style="text-align: center;"><strong><a href="http://chiragshah.org/" target="_blank">Prof. Chirag Shah</a> </br> University of Washington (USA)</strong></p>
			
			 <p><strong>Title</strong>: Investigating Bias and Instigating Fairness in Search and Recommendation.</p>
			
			 <p><strong>Abstract</strong>:
			 Bias is omnipresent -- from data to algorithms, and from framing of a problem to interpreting its solution. In this talk, I will highlight how such bias in general with machine learning techniques, and in particular with search and recommender systems cause material problems for users, businesses, and society at large. The examples span areas of search, education, and health. I will then introduce the idea of marketplace as a way to find a balance or fairness in the system and address the issue of bias, among other things. I will draw specific examples from our work on search and recommendation systems to demonstrate that achieving fairness in a marketplace and addressing bias in data and algorithms are not just morally and ethically right things to do, but could also lead to a more sustainable growth for various industries, governments, and our scientific advancement.</p>

			 <p><strong>About the speaker</strong>:
			 Chirag Shah is an Associate Professor in Information School (iSchool) at University of Washington (UW) in Seattle. Before UW, he was a faculty at Rutgers University. His research interests include studies of interactive information retrieval/seeking, trying to understand the task a person is doing and providing proactive recommendations. In addition to creating task-based IR systems that provide more personalized reactive and proactive recommendations, he is also focusing on making such systems transparent, fair, and free of biases. Dr. Shah received his MS in Computer Science from University of Massachusetts (UMass) at Amherst, and PhD in Information Science from University of North Carolina (UNC) at Chapel Hill. His research is supported by grants from National Science Foundation (NSF), National Institute of Health (NIH), Institute of Museum and Library Services (IMLS), Amazon, Google, and Yahoo. He spent his sabbatical in 2018 at Spotify working on voice-based search and recommendation problems. In 2019, as an Amazon Scholar, he worked with Amazon’s Personalization team on applications involving personalized and task-oriented recommendations. He is the recipient of Microsoft BCS/BCS IRSG Karen Spärck Jones Award 2019. More information about Dr. Shah can be found at http://chiragshah.org/.</p>
        </div>
      </div>

    </section>

    <!--==========================
    Program Section
    ============================-->
    <section id="program" class="wow fadeInUp">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Final Program</h2>
		  
		  <p>The ongoing worldwide COVID-19 situation pushed ECIR 2020 organizers to make both the conference and the workshops an <strong><font color="red">open online event</font></strong> through Zoom.</p> 
		  
		  
		  <div style="margin: 10px auto 30px auto; width: 50%;">
			  <table class="table">
				  <tbody>
					<tr>
					  <th scope="col"><a href="https://us04web.zoom.us/j/464990256" target="_blank"><img src="https://www.csulb.edu/sites/default/files/groups/instructional-design/zoom-logo.png" alt="Zoom" style="width:138px;"></a></th>
				
					  <td scope="col">As chairs of Bias2020@ECIR Workshop - International Workshop on Algorithmic Bias in Search and Recommendation, we are pleased to invite you to the online version of the workshop. The virtual room will open at <strong><a href="https://us04web.zoom.us/j/464990256" target="_blank">https://us04web.zoom.us/j/464990256</a></strong> on <strong>Apr 14, 2020 08:45 - Lisbon Local Time  </strong> for anyone who wish to attend the workshop (talks, demos, keynote, and so on) and participate to the open discussion on this important research path.</br><strong>Please contact by email the Workshop's Chairs to obtain access. </strong></td>
					</tr>
				  </tbody>
				  <tr>
				  </tr>
			  </table>
		  </div>
		  
		  
		  
		  <p><strong><font color="red">All workshop-day timings refer to April 14, 2020 - Lisbon Local Time.</font></strong></p>
		  <div style="margin: 10px auto 30px auto; width: 50%;">
			  <table class="table">
				  <thead>
					<tr>
					  <th scope="col">Timing</th>
					  <th scope="col">Content</th>
					</tr>
				  </thead>
				  <tbody>
					<tr>
					  <th scope="row"><strong>09:00 - 09:20</strong></th>
					  <td><strong>Welcome Message and Connection Setup</strong></td>
					</tr>
					<tr>
					  <th scope="row"><strong>09:20 - 10:50</strong></th>
					  <td><strong>Bias Session #1</strong></td>
					</tr>
					<tr>
					  <th scope="row">09:20 - 09:45</th>
					  <td><a href="https://drive.google.com/open?id=1nvcyWD8CqJ7iHYpM_1yfH8ZT6KqA_HRx" target="_blank">Bias Goggles - Exploring the bias of Web Domains through the Eyes of the Users</a><br>
					      G. Konstantakis, I. Promponas, M. Dretakis, P. Papadakos</td>
					</tr>
					<tr>
					  <th scope="row">09:45 - 10:05</th>
					  <td><a href="https://drive.google.com/open?id=1o2sIVJne0TeaAlExmGurFgEv5isXRfx7" target="_blank">Facets of Fairness in Search and Recommendation</a> <br>
					      S. Verma, R. Gao, C. Shah</td>
					</tr>
					<tr>
					  <th scope="row">10:05 - 10:30</th>
					  <td><a href="https://drive.google.com/open?id=1c1SlEWYCZgXBRnJtQG3wM4C08ehoUqBH" target="_blank"> Mitigating Gender Bias in Machine Learning Data Sets</a> <br>
					      S. Leavy, G. Meaney, K. Wade, D. Greene</td>
					</tr>
					<tr>
					  <th scope="row">10:30 - 10:50</th>
					  <td><a href="https://drive.google.com/open?id=1RR3mDNk_L2wgM9g598Ky_rFU-Yjdvjm0" target="_blank">Why do we need to be bots? What prevents society from detecting biases in recommendation systems</a> <br>
					      T. D. Krafft, M. P. Hauer, K. A. Zweig</td>
					</tr>
					<tr>
					  <th scope="row"><strong>10:50 - 11:20</strong></th>
					  <td><strong>Coffee Break</strong></td>
					</tr>
					<tr>
					  <th scope="row"><strong>11:20 - 12:50</strong></th>
					  <td><strong>Bias Session #2</strong></td>
					</tr>
					<tr>
					  <th scope="row"><strong>11:20 - 11:45</strong></th>
					  <td><a href="https://drive.google.com/open?id=1j2vcTOkfTs9TpukdtyLdM8eBF61xkCCq" target="_blank">Matchmaking Under Fairness Constraints: a Speed Dating Case Study</a><br>
					      D. Paraschakis, B. J. Nilsson</td>
					</tr>
					<tr>
					  <th scope="row"><strong>11:45 - 12:05</strong></th>
					  <td><a href="https://drive.google.com/open?id=1IiDOJvplekIvYUyB-Ai5TS8QtNCGhq0I" target="_blank">Effect of Debiasing on Information Retrieval</a><br> 
					      E. Gerritse, A. de Vries</td>
					</tr>
					<tr>
					  <th scope="row"><strong>12:05 - 12:30</strong></th>
					  <td><a href="https://drive.google.com/open?id=1v8_nRaj2TIgQXCKSUGHwrgIwL1_cGMmK" target="_blank">Using String-Comparison measures to Improve and Evaluate Collaborative Filtering Recommender Systems</a> <br> 
					      L. M. Lustosa Pascoal, H. A. Dantas Do Nascimento, T. Couto Rosa, E. Queiroz da Silva, E. Lima Aleixo</td>
					</tr>
					<tr>
					  <th scope="row"><strong>12:30 - 12:50</strong></th>
					  <td><a href="https://drive.google.com/open?id=1PsEqXt3E_X7hvzur7J9k1dm9l0V4sxCg" target="_blank">Recommendation filtering à la carte for intelligent tutoring systems</a><br>
					      W. Perreira, M. Spalenza, J.-R. Bourguet, E. de Oliviera</td>
					</tr>
					<tr>
					  <th scope="row"><strong>12:50 - 14:00</strong></th>
					  <td><strong>Lunch Break</strong></td>
					</tr>
					
					<tr>
					  <th scope="row"><strong>14:00 - 15:00</strong></th>
					  <td> 
						<strong>Keynote (Main Conference)</strong> <br>
						<a href="https://ecir2020.org/program/">Focusing the macroscope: how we can use data to understand behavior</a><br>
						Joana Gonçalves de Sá
					  </td>
					</tr>
					
					<tr>
					  <th scope="row"><strong>15:00 - 15:15</strong></th>
					  <td><strong>Coffee Break</strong></td>
					</tr>
					
					
					<tr>
					  <th scope="row"><strong>15:15 - 16:05</strong></th>
					  <td> 
						<strong>Keynote (BIAS Workshop)</strong> <br>
						<a href="https://drive.google.com/open?id=1sf3WByBSMXy2bWjToqqmtWVHqIhjUQPE" target="_blank">Investigating Bias and Instigating Fairness in Search and Recommendation </a><br>
						C. Shah
					  </td>
					</tr>

					
					<tr>
					  <th scope="row"><strong>16:05 - 16:50</strong></th>
					  <td><strong>Social Aspects Session #1</strong></td>
					</tr>
				
					<tr>
					  <th scope="row"><strong>16:05 - 16:50</strong></th>
					  <td>Analyzing the Interaction of Users with News Articles to Create Personalization Services <br>
					      A. Celi, R. Eramo, A. Piad, J. Diaz Blanco
						  <br> <br>
						  A Novel Similarity Measure for Group Recommender Systems with Optimal Time Complexity <br>
						  G. Ramos, C. Caleiro
						  <br> <br>
						  Improving News Personalization through Search Logs <br>
						  X. Bai, B. B. Cambazoglu, F. Gullo, A. Mantrach, F. Silvestri
						  <br> <br>
						  What kind of content are you prone to tweet? Multi-topic Preference Model for Tweeters <br> 
						  L. Recalde, R. Baeza-Yates
						  <br> <br>
						  Beyond Accuracy in Link Prediction <br>
						  J. Sanz-Cruzado, P. Castells						  
						  <br> <br>
						  Venue Suggestion Using Social-Centric Scores <br> 
						  M. Aliannejadi, F. Crestani
						  <br> <br>
						  Data Pipelines for Personalized Exploration of Rated Datasets <br>
						  S. Amer-Yahia, A. Tho Le, E. Simon
						  <br> <br>
						  The Impact of Foursquare Checkins on Users’ Emotions on Twitter <br>
						  S. A. Mirlohi Falavarjani, H. Hosseini, E. Bagheri
						  <br> <br>
						  Enriching Product Catalogs with User Opinions <br>
						  T. de Melo, A. da Silva, E. de Moura, P. Calado
						  </td>
					</tr>
					<tr>
					  <th scope="row"><strong>16:50 - 17:20</strong></th>
					  <td><strong>Open Discussion</strong></td>
					</tr>
					<tr>
					  <th scope="row"><strong>17:20 - 17:30</strong></th>
					  <td><strong>Concluding Remark</strong></td>
					</tr>
				  </tbody>
			  </table>
		  </div>
        </div>
      </div>

    </section>

    <!--==========================
    Important Dates Section
    ============================-->
    <section id="dates" class="wow fadeInUp section-with-bg">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Important Dates</h2>
          <ul>
            <li>Submissions: <del>January 27, 2020</del> <font color="red">February 3, 2020</font></li>
            <li>Notifications: <del>February 27, 2020</del> <font color="red">March 11, 2020 </font></li>
            <li>Camera-Ready: <del>March 30, 2020</del> <strong><font color="red">April 12, 2020 </font></strong></li>
            <li>Workshop: <strong><font color="red">April 14, 2020 - ONLINE </font> </strong></li>
          </ul>
          <p>All deadlines are 11:59pm, AoE time (Anywhere on Earth).</p>
        </div>
      </div>

    </section>

    <!--==========================
    Topics Section
    ============================-->
    <section id="topics" class="wow fadeInUp">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Topics</h2>
          <p>We solicit contributions in topics related to algorithmic bias in search and recommendation, focused (but not limited) to:</p>
          <ul>
            <li><strong>Data Set Collection and Preparation</strong>:
               <ul>
                  <li>Managing imbalances and inequalities within data sets</li>
                  <li>Devising collection pipelines that lead to fair and unbiased data sets</li>
                  <li>Collecting data sets useful for studying potential biased and unfair situations</li>
                  <li>Designing procedures for creating synthetic data sets for research on bias and fairness</li>
              </ul>
            </li>
            <li><strong>Countermeasure Design and Development</strong>:
               <ul>
                  <li>Conducting exploratory analysis that uncover biases</li>
                  <li>Designing treatments that mitigate biases (e.g., popularity bias mitigation)</li>
                  <li>Devising interpretable search and recommendation models</li>
                  <li>Providing treatment procedures whose outcomes are easily interpretable</li>
                  <li>Balancing inequalities among different groups of users or stakeholders</li>
              </ul>
            </li>
            <li><strong>Evaluation Protocol and Metric Formulation</strong>:
               <ul>
                  <li>Conducting quantitative experimental studies on bias and unfairness</li>
                  <li>Defining objective metrics that consider fairness and/or bias</li>
                  <li>Formulating bias-aware protocols to evaluate existing algorithms</li>
                  <li>Evaluating existing strategies in unexplored domains</li>
              </ul>
            </li>
            <li><strong>Case Study Exploration</strong>:
               <ul>
                  <li>E-commerce platforms</li>
                  <li>Educational environments</li>
                  <li>Entertainment websites</li>
                  <li>Healthcare systems</li>
                  <li>Social networks</li>
              </ul>
            </li>
          </ul>

        </div>
      </div>

    </section>

    <!--==========================
    Submission Details Section
    ============================-->
    <section id="submission" class="wow fadeInUp section-with-bg">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Submission Details</h2>
          <p>All submissions must be written in English. Authors should consult ECIR <a href="http://irsg.bcs.org/proceedings/ECIR_Draft_Guidelines.pdf" target="_blank">paper guidelines</a> and
             <a href="http://sigir.org/wp-content/uploads/2018/01/p032.pdf" target="_blank"> Fuhr’s guide to avoid common IR evaluation mistakes</a>, for the preparation of their papers. Authors should consult
             <a href="ftp://ftp.springernature.com/cs-proceeding/svproc/guidelines/Springer_Guidelines_for_Authors_of_Proceedings.pdf" target="_blank">Springer’s authors’ guidelines </a> and use their
             proceedings templates, either <a href="ftp://ftp.springernature.com/cs-proceeding/llncs/llncs2e.zip" target="_blank">LaTeX</a> or <a href="ftp://ftp.springernature.com/cs-proceeding/llncs/word/splnproc1703.zip" target="_blank"> Word</a>.
             Papers should be submitted as PDF files to Easychair at <a href="https://easychair.org/conferences/?conf=bias2020" target="_blank">https://easychair.org/conferences/?conf=bias2020</a>.
             Please be aware that at least one author per paper needs to register and attend the workshop to present the work.</p>
          <p>We will consider three different submission types:</p>
          <ul>
            <li><strong>Full papers (14 pages) </strong> should be clearly placed with respect to the state of the art and state the contribution of the proposal
                in the domain of application, even if presenting preliminary results. In particular, research papers should describe the methodology
                in detail, experiments should be repeatable, and a comparison with the existing approaches in the literature should be made. </li>
            <li><strong> Short papers (8 pages) </strong> should introduce new point of views in the workshop topics or summarize the experience of a researcher or a
                group in the field. Practice and experience reports should present in detail real-world scenarios in which search and recommender
                systems are exploited. </li>
            <li><strong>Demo papers (4 pages) </strong> should present a prototype or an application that employs search and recommender systems in the workshop topics.
                The systems will be shown at the workshop.</li>
          </ul>

          <p>Submissions should not exceed the indicated number of pages, including any diagrams and references. </p>
			 
		  <p>We expect authors, PC, and the organizing committee to adhere to the <a href="https://www.acm.org/special-interest-groups/volunteer-resources/acm-conflict-of-interest-policy" target="_blank">ACM’s Conflict of Interest Policy </a> and the <a href="https://www.acm.org/code-of-ethics" target="_blank">ACM’s Code of Ethics and Professional Conduct</a>.
</p>

          </div>
      </div>

    </section>

    <!--==========================
    Committee Section
    ============================-->
    <section id="committee" class="wow fadeInUp">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Committees</h2>
          <p><strong>Workshop Chairs</strong></p>
          <ul>
            <li><a href="https://www.ludovicoboratto.com/" target="_blank">Ludovico Boratto</a>, Eurecat - Centre Tecnológic de Catalunya (Spain)</li>
            <li><a href="https://www.unitelmasapienza.it/it/contenuti/personale/stefano-faralli" target="_blank">Stefano Faralli</a>, Unitelma Sapienza University of Rome (Italy)</li>
            <li><a href="https://www.mirkomarras.com/" target="_blank">Mirko Marras</a>, University of Cagliari (Italy)</li>
            <li><a href="https://scholar.google.com/citations?user=uTyaicMAAAAJ&hl=en" target="_blank">Giovanni Stilo</a>, University of L’Aquila (Italy)</li>
          </ul>
          <p><strong>Program Committee</strong></p>
          <ul>
            <li>Himan Abdollahpouri, University of Colorado Boulder (United States)</li>
			<li>Luca Aiello, Nokia Bell Labs (United Kingdom)</li>
			<li>Mehwish Alam, FIZ Karlsruhe - Karlsruhe Institute of Technology (Germany)</li>
			<li>Marcelo Armentano, National University of Central Buenos Aires (Argentina)</li>
			<li>Solon Barocas, Microsoft Research - Cornell University (United States)</li>
			<li>Alejandro Bellogin, Universidad Autónoma de Madrid (Spain)</li>
			<li>Asia Biega, Microsoft Research (United States)</li>
			<li>Glencora Borradaile, Oregon State University (United States) </li>
			<li>Federica Cena, University of Turin (Italy)</li>
			<li>Pasquale De Meo, University of Messina (Italy)</li>
			<li>Sarah Dean, University of California Berkeley (USA)</li>
			<li>Danilo Dessì, FIZ Karlsruhe - Karlsruhe Institute of Technology (Germany)</li>
			<li>Laura Dietz, University of New Hampshire (United States)</li>
			<li>Damiano Distante, Unitelma Sapienza University of Rome (Italy)</li>
			<li>Carlotta Domeniconi, George Mason University (United States)</li>
			<li>Michael Ekstrand, Boise State University (United States)</li>
			<li>Francesco Fabbri, Universitat Pompeu Fabra (Spain)</li>
			<li>Golnoosh Farnadi, Mila - University of Montreal (Canada)</li>
			<li>Nina Grgic-Hlaca, Max Planck Institute for Software Systems (Germany)</li>
			<li>Rossi Kamal, Kyung Hee University (South Korea)</li>
			<li>Toshihiro Kamishima, AIST (Japan)</li>
			<li>Karrie Karahalios, University of Illinois (United States)</li>
			<li>Aonghus Lawlor, University College Dublin (Ireland)</li>
			<li>Cataldo Musto, University of Bari Aldo Moro (Italy)</li>
			<li>Razieh Nabi, Johns Hopkins University (United States) </li>
			<li>Federico Nanni, The Alan Turing Institute (United Kingdom)</li>
			<li>Alexander Panchenko, Skolkovo Institute of Science and Technology (Russia)</li>
			<li>Panagiotis Papadakos, University of Crete (Greece)</li>
			<li>Emma Pierson, Stanford University (United States) </li>
			<li>Simone Paolo Ponzetto, Universität Mannheim (Germany)</li>
			<li>Alessandro Raganato, University of Helsinki (Finland)</li>
			<li>Babak Salimi, University of Washington (United States)</li>
			<li>Fabrizio Silvestri, Facebook (United Kingdom)</li>
			<li>Antonela Tommasel, National University of Central Buenos Aires (Argentina)</li>
			<li>Kyle Williams, Microsoft Research (United States)</li>
			<li>Eva Zangerle, University of Innsbruck (Austria)</li>
			<li>Markus Zanker, Free University of Bolzano-Bozen (Italy) </li>
			<li>Meike Zehlike, Max Planck Institute for Software Systems (Germany)</li>
			<li>Arkaitz Zubiaga, Queen Mary University of London (United Kingdom)</li>			
          </ul>
          </div>
      </div>

    </section>

    <!--==========================
    Contacts Section
    ============================-->
    <section id="contacts" class="wow fadeInUp section-with-bg">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Contacts</h2>
          <p>For general enquiries on the workshop, please send an email to <strong>ludovico.boratto@acm.org</strong>, <strong>stefano.faralli@unitelmasapienza.it</strong>,
             <strong>mirko.marras@unica.it</strong>, and <strong>giovanni.stilo@univaq.it</strong>.</p>
        </div>
      </div>

    </section>


  </main>

  <a href="#" class="back-to-top"><i class="fa fa-angle-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="lib/jquery/jquery.min.js"></script>
  <script src="lib/jquery/jquery-migrate.min.js"></script>
  <script src="lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/superfish/hoverIntent.js"></script>
  <script src="lib/superfish/superfish.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/venobox/venobox.min.js"></script>
  <script src="lib/owlcarousel/owl.carousel.min.js"></script>

  <!-- Contact Form JavaScript File -->
  <script src="contactform/contactform.js"></script>

  <!-- Template Main Javascript File -->
  <script src="js/main.js"></script>
</body>

</html>
