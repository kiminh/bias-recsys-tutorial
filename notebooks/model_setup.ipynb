{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook #1: Designing and evaluating a recommendation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the working environment\n",
    "\n",
    "- Python 3.6\n",
    "- Requirements: matplotlib, numpy, pandas, scikit-learn, scipy, tensorflow-gpu==2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not open requirements file: [Errno 2] No such file or directory: \"'../requirements.txt'\"\n",
      "You are using pip version 18.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install -r '../requirements.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.train_test_splitter import *\n",
    "from helpers.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path= '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train-test split parameters\n",
    "- **dataset**: csv file present in the data/datasets folder\n",
    "- **method**: 'uftime' for fixed timestamp split, 'utime' for time-based split per user, 'urandom' for random split per user \n",
    "- **percentage**: percentage of data to be included in the train set\n",
    "- **min_train**: minimum number of train samples for a user to be included  \n",
    "- **min_test**: minimum number of test samples for a user to be included\n",
    "- **min_time**: start time of interactions to be included\n",
    "- **max_time**: end time of interactions to be included\n",
    "- **step_time**: timestamp step while computing the fixed timestamp splitter (only for method \"uftime\") \n",
    "- **user_field**: name of the user column in the dataset csv file\n",
    "- **item_field**: name of the item column in the dataset csv file\n",
    "- **rating_field**: name of the rating column in the dataset csv file\n",
    "- **time_field**: name of the user column in the dataset csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ml1m'          \n",
    "method = 'utime'\n",
    "percentage = 0.80        \n",
    "min_train = 8\n",
    "min_test = 2\n",
    "min_time = None\n",
    "max_time = None\n",
    "step_time = 1000\n",
    "user_field = 'user_id'\n",
    "item_field = 'item_id'\n",
    "rating_field = 'rating'\n",
    "time_field = 'timestamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/datasets/' + dataset + '.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>type</th>\n",
       "      <th>type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2000-12-31 23:12:40</td>\n",
       "      <td>Drama</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1193</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2000-12-31 22:33:33</td>\n",
       "      <td>Drama</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>1193</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-12-31 00:49:39</td>\n",
       "      <td>Drama</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1193</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-12-30 19:01:19</td>\n",
       "      <td>Drama</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1193</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2000-12-30 07:41:11</td>\n",
       "      <td>Drama</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating            timestamp   type  type_id\n",
       "0        1     1193     5.0  2000-12-31 23:12:40  Drama        7\n",
       "1        2     1193     5.0  2000-12-31 22:33:33  Drama        7\n",
       "2       12     1193     4.0  2000-12-31 00:49:39  Drama        7\n",
       "3       15     1193     4.0  2000-12-30 19:01:19  Drama        7\n",
       "4       17     1193     5.0  2000-12-30 07:41:11  Drama        7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the train and tets split (splitting methods are defined in ./helpers/traintest_splitter.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Parsing user 6000 of 6040\n",
      "> Mean number of train ratings per learner: 133.07913907284768\n",
      "> Mean number of test ratings per learner: 32.51837748344371\n"
     ]
    }
   ],
   "source": [
    "if method == 'uftime':\n",
    "    traintest = fixed_timestamp(data, min_train, min_test, min_time, max_time, step_time, user_field, item_field, time_field, rating_field)\n",
    "elif method == 'utime':\n",
    "    traintest = user_timestamp(data, percentage, min_train+min_test, user_field, item_field, time_field)\n",
    "elif method == 'urandom':\n",
    "    traintest = user_random(data, percentage, min_train+min_test, user_field, item_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some statistics on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset\n",
      "> Interactions 1000209\n",
      "> Users 6040\n",
      "> Items 3706\n",
      "Train dataset\n",
      "> Interactions 803798\n",
      "> Users 6040\n",
      "> Items 3667\n",
      "Test dataset\n",
      "> Interactions 196411\n",
      "> Users 6040\n",
      "> Items 3532\n"
     ]
    }
   ],
   "source": [
    "print('Full dataset')\n",
    "print('> Interactions', len(traintest.index))\n",
    "print('> Users', len(np.unique(traintest[user_field].values)))\n",
    "print('> Items', len(np.unique(traintest[item_field].values)))\n",
    "\n",
    "print('Train dataset')\n",
    "print('> Interactions', len(traintest[traintest['set']=='train'].index))\n",
    "print('> Users', len(np.unique(traintest[traintest['set']=='train'][user_field].values)))\n",
    "print('> Items', len(np.unique(traintest[traintest['set']=='train'][item_field].values)))\n",
    "\n",
    "print('Test dataset')\n",
    "print('> Interactions', len(traintest[traintest['set']=='test'].index))\n",
    "print('> Users', len(np.unique(traintest[traintest['set']=='test'][user_field].values)))\n",
    "print('> Items', len(np.unique(traintest[traintest['set']=='test'][item_field].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save train and test sets in ./data/outputs/splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintest.to_csv(os.path.join(data_path, 'outputs/splits/' + dataset + '_' + method + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the experiment parameters\n",
    "- **dataset**: csv file present in the data/datasets folder\n",
    "- **method**: 'uftime' for fixed timestamp split, 'utime' for time-based split per user, 'urandom' for random split per user \n",
    "- **mode**: type of feedback to be used (i.e., 'implicit' or 'explicit')\n",
    "- **user_field**: name of the user column in the dataset csv file\n",
    "- **item_field**: name of the item column in the dataset csv file\n",
    "- **rating_field**: name of the rating column in the dataset csv file\n",
    "- **type_field**: name of the category id column in the dataset csv file\n",
    "- **model_type**: identifier of the recommendation model to test\n",
    "- **cutoffs**: comma-separated list of cutoffs to be used for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ml1m'\n",
    "method = 'utime'\n",
    "mode = 'implicit'\n",
    "user_field = 'user_id'\n",
    "item_field = 'item_id'\n",
    "rating_field = 'rating'\n",
    "type_field = 'type_id'\n",
    "cutoffs = '5,10,20,50,100,200'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-compute train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loaded 803798 train interactions\n",
      "> Loaded 196411 test interactions\n"
     ]
    }
   ],
   "source": [
    "traintest = pd.read_csv('../data/outputs/splits/' + dataset + '_' + method + '.csv', encoding='utf8')\n",
    "train = traintest[traintest['set']=='train'].copy()\n",
    "test = traintest[traintest['set']=='test'].copy()\n",
    "print('> Loaded', len(train.index), 'train interactions')\n",
    "print('> Loaded', len(test.index), 'test interactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some statistics on users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loaded 6040 users - 0 - 6039 - 6040 - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "> Loaded 3706 items - 0 - 3705 - 3706 - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "users = list(np.unique(traintest[user_field].values))\n",
    "items = list(np.unique(traintest[item_field].values))\n",
    "users.sort()\n",
    "items.sort()\n",
    "print('> Loaded', len(users), 'users -', np.min(users), '-', np.max(users), '-', len(np.unique(users)), '-', users[:10])\n",
    "print('> Loaded', len(items), 'items -', np.min(items), '-', np.max(items), '-', len(np.unique(items)), '-', items[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load category data for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Retrieved 3706 mapping indexes, one per course\n",
      "> Loading item categories identifiers - 18 categories like [ 7 13  3]\n"
     ]
    }
   ],
   "source": [
    "items_metadata = traintest.drop_duplicates(subset=['item_id'], keep='first')\n",
    "print('> Retrieved', len(items_metadata.index), 'mapping indexes, one per course')\n",
    "category_per_item = items_metadata[type_field].values\n",
    "print('> Loading item categories identifiers -', len(set(category_per_item)), 'categories like', category_per_item[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the type of feedback you want to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'implicit':\n",
    "    train[rating_field] = train[rating_field].apply(lambda x: 1.0)\n",
    "    test[rating_field] = test[rating_field].apply(lambda x: 1.0)\n",
    "    traintest[rating_field] = traintest[rating_field].apply(lambda x: 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model architecture to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'neumf'\n",
    "gen_mode = 'point' # pair for 'pair-wise' or point for 'point-wise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Initializing user, item, and categories lists\n",
      "> Initializing observed, unobserved, and predicted relevance scores\n",
      "> Initializing item popularity lists\n",
      "> Initializing category per item\n",
      "> Initializing category preference per user\n",
      "> Initializing metrics\n"
     ]
    }
   ],
   "source": [
    "from models.neumf import NeuMF\n",
    "\n",
    "model = NeuMF(users, items, train, test, category_per_item, item_field, user_field, rating_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training instances of type point\n",
      "> Making instances for interaction 800000 / 803798 of type point\n",
      "> Making training - Epochs 20 Batch Size 1024 Learning Rate 0.001 Factors 10 Negatives 10 Mode point\n",
      "Train on 7957600 samples, validate on 884178 samples\n",
      "Epoch 1/20\n",
      "7956480/7957600 [============================>.] - ETA: 0s - loss: 0.2086\n",
      "Epoch 00001: val_loss improved from inf to 0.23358, saving model to ../data/outputs/models/ml1m_utime_neumf_model.h5\n",
      "7957600/7957600 [==============================] - 79s 10us/sample - loss: 0.2086 - val_loss: 0.2336\n",
      "Epoch 2/20\n",
      "7956480/7957600 [============================>.] - ETA: 0s - loss: 0.1749- ETA: 0s - loss: \n",
      "Epoch 00002: val_loss did not improve from 0.23358\n",
      "7957600/7957600 [==============================] - 63s 8us/sample - loss: 0.1749 - val_loss: 0.2404\n",
      "Epoch 3/20\n",
      "7954432/7957600 [============================>.] - ETA: 0s - loss: 0.1666\n",
      "Epoch 00003: val_loss did not improve from 0.23358\n",
      "7957600/7957600 [==============================] - 89s 11us/sample - loss: 0.1666 - val_loss: 0.2380\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.train(os.path.join(data_path, 'outputs/models/' + dataset + '_' + method + '_' + model_type + '_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mlp_embedding_user (Embedding)  (None, 1, 32)        193280      user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlp_embedding_item (Embedding)  (None, 1, 32)        118592      item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 32)           0           mlp_embedding_user[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32)           0           mlp_embedding_item[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mf_embedding_user (Embedding)   (None, 1, 10)        60400       user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mf_embedding_item (Embedding)   (None, 1, 10)        37060       item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 32)           2080        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 10)           0           mf_embedding_user[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           mf_embedding_item[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 16)           528         layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 10)           0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer3 (Dense)                  (None, 8)            136         layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 18)           0           multiply[0][0]                   \n",
      "                                                                 layer3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Dense)              (None, 1)            19          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 412,095\n",
      "Trainable params: 412,095\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute user-item relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Making predictions for user 657 / 6040 / 6040 28 / 6040 29 / 6040 30 / 6040 32 / 6040 37 / 6040 43 / 6040 50 / 6040 56 / 6040 65 / 6040 66 / 6040 105 / 6040 122 / 6040 124 / 6040 125 / 6040 154 / 6040 155 / 6040 165 / 6040 184 / 6040 185 / 6040 186 / 6040 187 / 6040 188 / 6040 193 / 6040 201 / 6040 218 / 6040 248 / 6040 249 / 6040 277 / 6040 283 / 6040 304 / 6040 305 / 6040 335 / 6040 336 / 6040 337 / 6040 361 / 6040 362 / 6040 366 / 6040 367 / 6040 368 / 6040 397 / 6040 400 / 6040 432 / 6040 451 / 6040 459 / 6040 460 / 6040 461 / 6040 462 / 6040 491 / 6040 509 / 6040 515 / 6040 518 / 6040 521 / 6040 523 / 6040 533 / 6040 544 / 6040 546 / 6040 548 / 6040 565 / 6040 566 / 6040 578 / 6040 579 / 6040 582 / 6040 586 / 6040 598 / 6040 599 / 6040 606 / 6040 608 / 6040 613 / 6040 631 / 6040"
     ]
    }
   ],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(model.get_predictions(), os.path.join(data_path, 'outputs/predictions/' + dataset + '_' + method + '_' + model_type + '_pred.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Making metrics for user 6000 / 6040"
     ]
    }
   ],
   "source": [
    "model.test(cutoffs=np.array([int(k) for k in cutoffs.split(',')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(model.get_metrics(), os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + method + '_' + model_type + '_metr.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Precision: 0.135 \n",
      " Recall: 0.0319 \n",
      " NDCG: 0.1409 \n",
      " Hit Rate: 0.4106 \n",
      " Avg Popularity: 1679.0851 \n",
      " Category Diversity: 0.2066 \n",
      " Novelty: 2.0525 \n",
      " Item Coverage: 0.23 \n",
      " User Coverage: 0.4106\n"
     ]
    }
   ],
   "source": [
    "model.show_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}